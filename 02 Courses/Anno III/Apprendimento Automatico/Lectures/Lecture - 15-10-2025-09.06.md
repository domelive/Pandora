---
date: 2025-10-15 09.06
course: 02 Courses/Anno III/Apprendimento Automatico/Lectures
tags:
  - lecture
  - "#approccio_probabilistico"
reviewed: false
last_reviewed:
next_review:
---
## üß† Concetti
---
#### Ripasso Di Probabilita'
___Variabile Aleatoria___
+ Una variabile aleatoria $X$ _denota l'esito di un fenomeno riguardo al quale sussiste incertezza_, come ad esempio il risultato di un processo stocastico.
___Probabilita' Di Una Variabile Aleatoria___
+ Definiamo la probabilita' di $X$, $P(X)$, come la _frazione di volte per cui $X$ e' vero in esecuzioni ripetute dell'esperimento_.

Piu' formalmente:
+ L'insieme $\Omega$ dei _possibili esiti_ di un esperimento causale e' detto _spazio campionario_.
+ Una _variabile aleatoria_ e' una _funzione misurabile_ su $\Omega$.
+ Un _evento_ e' un qualche _sottoinsieme di_ $\Omega$.
+ Noi siamo interessati alla _probabilita' di eventi specifici_ $P(X)$, e alla _probabilita' condizionata da altri eventi_ $P(X|Y)$.
#### Spazio Campionario
La scelta dello spazio campionario richiede un po' di cautela.

Spesso la nostra intuizione della probabilita' si basa sul presupposto che gli eventi elementari nello spazio campionario abbiano la stessa probabilita', cosa che non e' necessariamente vera.

Si utilizza una visualizzazione grafica:![[Screenshot_20251015_095655.png]]
+ La _probabilita'_ $P(A)$ _dell'evento $A$ e' il rapporto tra l'area di $A$ e l'area dell'intero spazio campionario_, supposto uniforme.
#### Assiomi Della Teoria Della Probabilita'
+ $$0 \leq P(A) \leq 1$$
+ $$P(True) = 1$$
+ $$P(False) = 0$$ 
+ $$P(A \lor B)=P(A) + P(B) - P(A \land B)$$
#### Teoremi Derivati
+ $$P(\neg A)=1-P(A)$$
_Dimostrazione_:
+ Sappiamo che $$P(A \lor B)=P(A) + P(B) - P(A \land B)$$e in particolare $$P(A \lor \neg A)=P(A) + P(\neg A) - P(A \land \neg A)$$ma $$P(A \lor \neg A) = P(True)=1 \ \ \ \ \ \text{e} \ \ \ \ \ P(A \land \neg A)=P(False)=0$$e quindi $$1 = P(A) + P(\neg A)$$Q.E.D.

+ $$P(A)=P(A\land B)+P(A\land \neg B)$$
_Dimostrazione_:
+ Sappiamo che $$P(A \lor B)=P(A) + P(B) - P(A \land B)$$siccome $$A = A\land (B \lor \neg B)=(A\land B)\lor(A\land \neg B)$$abbiamo $$\begin{align*}
P(A) &= P(A \land B) + P(A \land \lnot B) - P((A \land B) \land (A \land \lnot B)) \\
     &= P(A \land B) + P(A \land \lnot B) - P(\text{False}) \\
     &= P(A \land B) + P(A \land \lnot B)
\end{align*}$$
#### Variabili Aleatorie Multivalore
$A$ e' una _variabile aleatoria discreta a $k$-valori_ se puo' _assumere esattamente uno dei valori_ $\{\nu_{1},\nu_{2},\dots,\nu_{k}\}$.

$P(A=\nu_{i})$ e' la _probabilita' che un elemento dello spazio campionario abbia valore_ $A = \nu _i$.
+ $P(A=\nu_{i}\land A=\nu_{j})=0$ se $i \neq j$.
+ $P(A=\nu_{1}\lor A=\nu_{2}\lor\dots \lor A=\nu_{k})=1$.
#### Probabilita' Condizionata
La probabilita' condizionata dell'evento $A$ _dato_ l'evento$B$ e' _definita come la quantita'_ $$P(A|B)=\frac{P(A\land B)}{P(B)}$$Da cui possiamo derivare (corollario) la _Chain Rule_ $$P(A\land B)=P(B)\cdot P(A|B)=P(A)\cdot P(B|A)$$
#### Eventi Indipendenti
Due eventi $A$ e $B$ sono detti _indipendenti se_ $$P(A|B)=P(A)$$ovvero, _l'evento $B$ non ha influenza sull'evento $A$_.

Dalla Chain Rule possiamo ricavare il seguente _corollario_: $$P(A\land B)=P(A)\cdot P(B)$$
Inoltre, siccome $$P(A\land B)=P(B|A)\cdot P(A)$$abbiamo anche che $$P(B|A)=P(B)$$cioe' _anche l'evento $B$ e' indipendente da $A$_.

Di seguito l'intuizione grafica:![[Screenshot_20251015_103525.png|700]]
#### La Regola Di Bayes
Di seguito la prima formulazione della regola di Bayes: $$P(A|B)=\frac{P(A)\cdot P(B|A)}{P(B)}$$
La regola di Bayes puo' anche essere scritta con un'altra formulazione. Di seguito i passaggi:
+ Si parte dalla formulazione sopra: $$P(Y|X)=\frac{P(Y)\cdot P(X|Y)}{P(X)}$$ed in particolare, per ogni $i,j$ $$P(Y=y_{i}|X=x_{j})=\frac{P(Y=y_{i})\cdot P(X=x_{j}|Y=y_{i})}{P(X=x_{j})}$$Ma sappiamo che $$P(X=x_{j})=\sum^{}_{i}P(X=x_{j},Y=y_{i})=\sum^{}_{i}P(Y=y_{i})\cdot P(X=x_{j}|Y=y_{i})$$e quindi $$P(Y=y_{i}|X=x_{j})=\frac{P(Y=y_{i})\cdot(X=x_{j}|Y=y_{i})}{\sum^{}_{i}P(Y=y_{i})\cdot P(X=x_{j}|Y=y_{i})}$$
Data la seconda formulazione possiamo distinguere _posterior, likelihood, prior e marginal_: $$\underbrace{P(Y|X)}_{posterior}=\frac{\overbrace{P(X|Y)}^{likelihood}\cdot \overbrace{P(Y)}^{prior}}{\underbrace{P(X)}_{marginal \ likelihood}}=\frac{\overbrace{P(X|Y)}^{likelihood}\cdot \overbrace{P(Y)}^{prior}}{\underbrace{\sum^{}_{Y}P(X|Y)\cdot P(Y)}_{marginal \ likelihood}}$$
## ‚ùì Domande
---

## üí° Riferimenti
---

## üß© Tasks
---
+ [ ] Review [[Lecture - 15-10-2025-09.06]]
+ [ ] Cambiare data [[Lecture - 15-10-2025-09.06]]

```button 
name ‚úÖ Mark [[Lecture - 15-10-2025-09.06]] As Reviewed 
type command 
action QuickAdd: Mark As Reviewed
```
