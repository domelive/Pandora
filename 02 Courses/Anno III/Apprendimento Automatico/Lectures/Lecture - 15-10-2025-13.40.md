---
date: 2025-10-15 13.39
course: 02 Courses/Anno III/Apprendimento Automatico/Lectures
tags:
  - lecture
  - approccio_probabilistico
reviewed: false
last_reviewed:
next_review:
---
## üß† Concetti
---
#### Classificazione Di Documenti - Approccio Bag Of Words
Nella classificazione di documenti, gli _eventi elementari sono le parole che occorrono in posizione $i$ nel documento_.
Su ciascuna di queste parole, _definiamo una variabile aleatoria_ $X_{i}$ _che assume tanti possibili valori quante sono le parole nel vocabolario_. $$\theta_{i,\text{word}, l}=P(X_{i}=\text{word}|Y=l)$$indica la _probabilita' che nel documento della categoria $l$ la parola $\text{word}$ appare in posizione $i$_.
#### Training E Classificazione - Approccio Bag Of Words
Date le variabile aleatorie discrete $X_{i}, Y$.

___Training___:
+ Per ogni possible valore di $y_{k}$ di $Y$, _stimiamo_ $$\pi_{k}=P(Y=y_{k})$$ovvero la _probabilita' a priori di appartenere alla categoria_ $y_{k}$.
+ Per ogni possibile valore $x_{ij}$ dell'attributo $X_{i}$ _stimiamo_ $$\theta_{ijk}=P(X_{i}=x_{ij}|Y=y_{k})$$
___Classificazione___:
+ Per classificare una _sequenza di $n$ parole_ $a^{new}=\langle a_{1},\dots,a_{n} \rangle$ si calcola $$\begin{align*} Y^{new}&= \arg \max_{y_{k}}P(Y=y_{k})\cdot \prod^{}_{i}P(X_{i}=a_{i}|Y=y_{k})\\
&= \arg \max_{k}\pi_{k}\cdot \prod^{}_{i}\theta_{ijk} \end{align*}$$dove $x_{ij}=a_{i}$.
#### Stima Dei Parametri - Approccio Bag Of Words
Per la stima dei parametri si utilizzano le _[[Lecture - 15-10-2025-13.39#Riguardo La Maximum Likelihood|Maximum Likelihood Estimates]]_.
+ La _frazione dei documenti nella categoria_ $y_{k}$: $$\pi_{k}=P(Y=y_{k})$$
+ La _frequenza della parola $\text{word}$ nei documenti della categoria $y_{k}$_: $$\theta_{i,\text{word},k}=\theta_{\text{word},k}=P(X=\text{word}|Y=y_{k})$$
#### Log Likelihood
Invece di $$\begin{align*} Y^{new}&= \arg \max_{y_{k}}P(Y=y_{k})\cdot \prod^{}_{i}P(X_{i}=w_{j}|Y=y_{k})\\
&= \arg \max_{k}\pi_{k}\cdot \prod^{}_{i}\theta_{ijk} \end{align*}$$
Possiamo calcolare $$\begin{align*} Y^{new}&= \arg \max_{y_{k}}\log\left( P(Y=y_{k})\cdot \prod^{}_{i}P(X_{i}=a_{i}|Y=y_{k}) \right)\\
&= \arg \max_{k}\log(\pi_{k})\cdot \sum^{}_{i}\log(\theta_{ijk}) \end{align*}$$Inoltre, se $\theta_{ijk}=\theta_{i^{\prime}jk}=\theta_{jk}$ $$\sum^{}_{i}\log(\theta_{ijk})=\sum^{}_{j}n_{j}\cdot \log(\theta_{jk})$$
#### Correlazione
Consideriamo dei _vettori della stessa dimensione del vocabolario_.

___Training___
+ Per _ogni categoria $k$ dei documenti costruiamo un vettore 'spettrale'_ $$s_{k}=\langle \log(\theta_{jk}) \rangle_{j \in \text{words}}$$dove $\theta_{jk}$ e' la _frequenza della parola $j$ nei documenti della categoria $k$_.

___Classificazione___
+ Dato un _nuovo documento_, calcoliamo un vettore $$d=\langle n_{j} \rangle_{j \in \text{words}}$$
+ _classifichiamo_ il documento _in base alla categoria il cui spettro e' maggiormente correlato a questo vettore_ $$\arg \max_{k}\underbrace{d\cdot s_{k}}_{\text{correlazione}}=\sum^{}_{j}d_{j}\cdot s_{jk}$$
#### Prodotto Scalare E Similitudine Del Coseno
___Definizione geometrica del prodotto scalare___: 
+ Sia $\theta$ _l'angolo tra i due vettori_ $a,b$: $$a\cdot b=|a||b|\cos(\theta)$$

___Definizione Analitica___:
+ Dati $a=(a_{1},\dots,a_{n})$ e $b=(b_{1},\dots,b_{n})$ $$a\cdot b=\sum^{n}_{i=1}a_{i}b_{i}$$
___Similitudine Del Coseno___:
+ La similitudine del coseno (_cosine similarity_) tra $a$ e $b$ e' il _prodotto scalare normalizzato rispetto alla lunghezza dei vettori_: $$S_{C}(a,b)=\frac{a\cdot b}{|a||b|}=\cos(\theta)$$
___Equivalenza Della Definizione Geometrica E Analitica___:
+ Per la _regola del coseno_: $$|a-b|^{2}=|a|^{2}+|b|^{2}-2|a||b|\cos(\theta)$$
+ Quindi $$a\cdot b=\frac{|a|^{2}+|b|^{2}-|a-b|^{2}}{2}$$
+ Nel _caso planare_:
	+ Sia $a = (a_{1},a_{2})$ e $b=(b_{1},b_{2})$. Abbiamo $$a\cdot b=\frac{a_{1}^{2}+a_{2}^{2}+b_{1}^{2}+b_{2}^{2}-(a_{1}-b_{1})^{2}-(a_{2}-b_{2})^{2}}{2}=a_{1}b_{1}-a_{2}b_{2}$$
#### La Natura Lineare Di [[Lecture - 15-10-2025-13.39#Na√Øve Bayes|Na√Øve Bayes]] - Caso Booleano
Dati $X_{i}, Y$ _booleani_, la _classificazione_ di un vettore e' la seguente: $$\frac{P(Y=1|X_{1},\dots,X_{n}=\vec{x})}{P(Y=0|X_{1},\dots,X_{n}=\vec{x})}=\frac{P(Y=1)\prod^{}_{i}P(X_{i}=x_{i}|Y=1)}{P(Y=0)\prod^{}_{i}P(X_{i}=x_{i}|Y=0)} \geq 1$$
_Passando ai logaritmi_ si ottiene $$\log\left(\frac{P(Y=1)}{P(Y=0)}\right)+\sum^{}_{i}\log\left(\frac{P(X_{i}=x_{i}|Y=1)}{P(X_{i}=x_{i}|Y=0)}\right)\geq 0$$
Utilizziamo il fatto che, per una variabile booleana $x$: $$f(x)=x\cdot f(1)+(1-x)\cdot f(0)$$
Posto $\theta_{ik}=P(X_{i}=1|Y=k)$ (da cui $P(X_{i}=0|Y=k)=1-\theta_{ik}$) la _disequazione precedente diventa_: $$\log\left(\frac{P(Y=1)}{P(Y=0)}\right)+\sum^{}_{i}x_{i}\cdot \log\left(\frac{\theta_{i_{1}}}{\theta_{i_{0}}}\right)+\sum^{}_{i}(1-x_{i})\cdot \log\left(\frac{1-\theta_{i_{1}}}{1-\theta_{i_{0}}}\right)\geq 0$$che e' _lineare nelle features_.
#### Tecniche Di Classificazione Lineari
Sono _algoritmi di classificazione basati su una combinazione lineare delle features_.

Ogni _caratteristica del dato e' valutata indipendentemente dalle altre_ e _contribuisce al risultato in modo lineare con un peso opportuno_.

Questo _peso e' un parametro del modello che deve essere stimato_.

## ‚ùì Domande
---

## üí° Riferimenti
---

## üß© Tasks
---
+ [ ] Review [[Lecture - 15-10-2025-13.39]]
+ [ ] Cambiare data [[Lecture - 15-10-2025-13.39]]

```button 
name ‚úÖ Mark [[Lecture - 15-10-2025-13.39]] As Reviewed 
type command 
action QuickAdd: Mark As Reviewed
```
 