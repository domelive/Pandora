---
date: 2025-10-15 13.39
course: 02 Courses/Anno III/Apprendimento Automatico/Lectures
tags:
  - lecture
  - approccio_probabilistico
reviewed: false
last_reviewed:
next_review:
---
## üß† Concetti
---
#### La Distribuzione Congiunta
Si seguono i seguenti passaggi:
1. Costruire una _tabella con tutte le possibili combinazioni dei valori delle features_.
2. Stimare la _probabilita' per ogni combinazione di valori_.

Date $n$ features, _dobbiamo stimare_ $2^{n}-1$ _parametri_.

Disponendo della distribuzione congiunta, _possiamo stimare la probabilita' di qualunque evento esprimibile come combinazione logica delle features_ $$P(E)=\sum^{}_{row\in E}P(row)$$
E' altrettanto semplice _calcolare la probabilita' condizionata di un evento_ $E_{1}$ _dato un altro evento $E_{2}$_ $$P(E_{1}|E_{2})=\frac{P(E_{1}\land E_{2})}{P(E_{2})}=\frac{\sum^{}_{row \in E_{1}\land E_{2}}P(row)}{\sum^{}_{row\in E_{2}}P(row)}$$
#### Na√Øve Bayes
Suppone che $$P(X_{1},X_{2},\dots,X_{n}|Y)=\prod^{}_{i}P(X_{i}|Y)$$ovvero che, _dato $Y$, $X_{i}$ siano $X_{j}$ indipendenti tra loro_.
#### Indipendenza Condizionale
Due eventi $X_{i}$ e $X_{j}$ sono _indipendenti dato_ $Y$ se $$P(X_{i}|X_{j},Y)=P(X_{i}|Y)$$
#### Indipendenza Condizionale In Na√Øve Bayes
$$\begin{align*} P(X_{1},X_{2}|Y) &= P(X_{1}|X_{2},Y)\cdot P(X_{2}|Y) \ \ \ \text{per Chain Rule}\\
&= P(X_{1}|Y)\cdot P(X_{2}|Y) \ \ \ \text{per indipendenza condizionale} \end{align*}$$
In generale: $$P(X_{1},X_{2},\dots,X_{n}|Y)=\prod^{}_{i}P(X_{i}|Y)$$
#### Classificazione Di Un Nuovo Dato $x^{new}=\langle x_{1},\dots,x_{n} \rangle$
$$Y^{new}=\arg \max_{y_{i}}P(Y=y_{i})\cdot \prod^{}_{j}P(X_{j}=x_{j}|Y=y_{i})$$
#### Tecnica Di Apprendimento - Stima Dei Parametri
Date variabili _aleatorie discrete_ $X_{i}, Y$:
+ ___Training___
	+ Per tutti i valori $y_{k}$ di $Y$ stimare $$\pi_{k}=P(Y=y_{k})$$
	+ Per ogni possibile valore $x_{ij}$ di $X_{i}$ stimare $$\theta_{ijk}=P(X_{i}=x_{ij}|Y=y_{k})$$
+ ___Classificazione Di Un nuovo Dato___
	+ Si calcola $$\begin{align*} Y^{new} &= \arg \max_{y_{k}}P(Y=y_{k})\cdot \prod^{}_{i}P(X_{i}=a_{i}|Y=y_{k}) \\ &= \arg \max_{x_{k}}\pi \cdot \prod^{}_{i}\theta_{ijk}\end{align*}$$supponendo che $a_{i}=x_{ij}$, cioe' che $a_{i}$ sia il $j$-esimo tra i possibili valori discreti dell'attributo $X_{i}$.
+ ___Maximum Likelihood Estimates (MLE)___
	+ $$\pi_{k}=P(Y=y_{k})=\frac{\#\mathcal{D}\{ Y=y_{k} \}}{|\mathcal{D}|}$$
	+ $$\theta_{ijk}=P(X=x_{ij}|Y=y_{k})=\frac{\#\mathcal{D}\{ X_{i}=x_{ij}\land Y=y_{k} \}}{\#\mathcal{D}\{ Y=y_{k} \}}$$
## ‚ùì Domande
---

## üí° Riferimenti
---

## üß© Tasks
---
+ [ ] Review [[Lecture - 15-10-2025-13.39]]
+ [ ] Cambiare data [[Lecture - 15-10-2025-13.39]]

```button 
name ‚úÖ Mark [[Lecture - 15-10-2025-13.39]] As Reviewed 
type command 
action QuickAdd: Mark As Reviewed
```
