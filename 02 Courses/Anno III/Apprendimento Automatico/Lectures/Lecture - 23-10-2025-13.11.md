---
date: 2025-10-23 13.11
course: 02 Courses/Anno III/Apprendimento Automatico/Lectures
tags: lecture
reviewed: false
last_reviewed:
next_review:
---
## üß† Concetti
---
#### Gradiente
+ Definizione: √® il vettore delle derivate parziali. $$\nabla_{w}[E]=\left[\frac{\partial E}{\partial w_{1}},\dots,\frac{\partial E}{\partial w_{n}}\right]$$
	+ Training: $$\Delta w_{i}=\mu \cdot \frac{\partial E}{\partial w_{i}}$$dove $w_{i}=w_{i}+\Delta w_{i}$.
+ Cerco di capire quale direzione mi fa guadagnare pi√π rapidamente.
	+ La derivata parziale mi dice quanto rapidamente sto scendendo in quella direzione.
+ Devo trovare la direzione di massima discesa.
	+ Questo vettore √® locale.
		+ Mi dice in che direzione devo scendere per quel passaggio.
	+ Si usano delle euristiche.
		+ Ovvero il learning rate.
			+ Quando cambiamo questo parametro cambiamo il passo che facciamo nella direzione del gradiente prima di applicarlo.
+ Non garantisce la convergenza ad un punto di minimo globale.
	+ Algoritmo locale.
+ Funziona bene se ho delle ipotesi che mi dicono che la mia loss function sia convessa.
#### Approccio Iterativo
+ Si minimizza qualche funzione di errore $\Theta(w)$ misurata sul training set, aggiustando opportunamente i parametri del modello.
	+ Prende un punto a caso del piano vicino allo zero, e inizia a calcolare il gradiente da quel punto.
	+ Attraversiamo sempre le curve di livello perpendicolarmente.
+ Da recuperare ...

## ‚ùì Domande
---

## üí° Riferimenti
---

## üß© Tasks
---
+ [ ] Riscrivere E Recuperare [[Lecture - 23-10-2025-13.11]]
+ [ ] Review [[Lecture - 23-10-2025-13.11]]

```button 
name ‚úÖ Mark [[Lecture - 23-10-2025-13.11]] As Reviewed 
type command 
action QuickAdd: Mark As Reviewed
```
