---
date: 2025-10-16 13.09
course: 02 Courses/Anno III/Apprendimento Automatico/Lectures
tags:
  - lecture
  - "#approccio_probabilistico"
reviewed: false
last_reviewed:
next_review:
---
## üß† Concetti
---
#### [[Lecture - 15-10-2025-13.39#Na√Øve Bayes|Na√Øve Bayes]] Gaussiano
Se le _features sono_ $X_{i}$ _continue_, per utilizzare Na√Øve Bayes dobbiamo calcolare $P(X_{i}|Y)$ ma dato che le features sono continue, _le probabilita' puntuali sono nulle, per cui si parla di densita' delle distribuzioni_.

Un approccio tradizionale consiste nel _supporre che_ $P(X_{i}|Y)$ _abbia una distribuzione gaussiana_, detta anche normale.
+ ___Funzione Di Densita' Di Probabilita'___ (il suo integrale e' uguale a $1$): $$p(x|\mu,\sigma)=\frac{1}{\sigma \sqrt{ 2\pi }}e^{\frac{-(x-\mu)^{2}}{2\sigma^2}}$$
	+ _Valore medio_: $E[X]=\mu$.
	+ _Varianza_: $Var[X]=\sigma^{2}$.
	+ _Deviazione standard_: $\sigma_{X}=\sigma$.
#### True Positives/False Positives E True Negatives/False Negatives
![[Screenshot_20251018_163504.png]]
Dall'esempio nella figura, dove la _curva blu indica la distribuzione di una classe negativa_, la _curva rossa indica la distribuzione di una classe positiva_, e la _linea centrale tratteggiata indica il decision boundary_.
+ Tutti i valori a _sinistra_ del decision boundary sono classificati come negativi.
+ Tutti i valori a _destra_ del decision boundary sono classificati come positivi.

Si individuano:
+ _True Negatives_ ($TN$): campioni che appartengono alla _classe negativa_ e sono _correttamente classificati come negativi_.
	+ Si trovano sotto la curva blu e a sinistra della soglia.
+ _False Negatives_ ($FN$): campioni che appartengono alla _classe positiva_ ma che sono stati _scorrettamente classificati come negativi_.
	+ Si trovano sotto la curva arancione e a sinistra della soglia.
+ _True Positives_ ($TP$): campioni che appartengono alla _classe positiva_ e sono _correttamente classificati come positivi_.
	+ Si trovano sotto la curva arancione a destra della soglia.
+ _False Positives_ ($FP$): campioni che appartengono alla _classe negativa_ ma che sono stati _scorrettamente classificati come positivi_.
	+ Si trovano sotto la curva blu a destra della soglia.

Sostanzialmente questa figura mostra come un classificatore divide i dati di due gaussiane con una soglia.
A causa della sovrapposizione, alcuni esempio vengono classificati in modo errato.
+ La scelta della soglia determina il compromesso tra questi errori.
#### Accuratezza, Precisione E Richiamo
+ ___Accuratezza___: rappresenta il _numero di istanze classificate correttamente_. $$\text{Accuratezza}=\frac{TP+TN}{All}$$
+ ___Precisione___: rappresenta _quanto e' precisa la classificazione sui positivi_. $$\text{Precisione}=\frac{TP}{TP+FP}$$
+ ___Richiamo___: rappresenta _la percentuale dei positivi recuperata_. $$\text{Richiamo}=\frac{TP}{TP+FN}$$
+ ___Media Armonica Tra Precisione E Richiamo___: $$F_{1}=2\frac{\text{Precisione} \cdot \text{Richiamo}}{\text{Precisione} + \text{Richiamo}}$$
Possiamo anche vederlo con una visualizzazione intuitiva:
![[Screenshot_20251018_165311.png|center]]
#### Parametri Descrittivi Del Modello
_Assumiamo_ che per ogni valore $y_{k}$ di $Y$ la variabile aleatoria $P(X_{i}|Y=y_{k})$ abbia una _distribuzione gaussiana_ $$N(x|\mu_{ik},\sigma_{ik})=\frac{1}{\sigma_{ik}\sqrt{ 2\pi }}e^{\frac{(x-\mu_{jk})^{2}}{2\sigma^{2}_{{ik}}}}$$
___Apprendimento___:
+ Si _stimano i valori dei parametri_ $\mu_{ik},\sigma_{ik}$ e $\pi_{k}=P(Y=y_{k})$.
___Classificazione___: 
+ Troviamo un _nuovo dato_ $x^{new}=\langle a_{1},\dots,a_{n} \rangle$ $$\begin{align*}
Y^{new}&= \arg \max_{yk}P(Y=y_{k})\cdot \prod^{}_{i}P(X_{i}=a_{i}|Y=y_{k})\\
&= \arg \max_{k}\pi_{k}\cdot \prod^{}_{i}N(a_{i}|\mu_{ik},\sigma_{ik})
\end{align*}$$
___Maximum Likelihood Estimates___: 
+ $\mu_{ik}$ e' il _valore medio di_ $X_{i}$ _per i dati con etichetta_ $Y=y_{k}$. Formalmente: $$\mu_{ik}=\frac{\sum^{}_{j}X_{i}^{j}\delta(Y^{j}=y_{k})}{\sum^{}_{j}\delta(Y^{j}=y_{k})}$$dove $j$ _varia sulle istanze_ del training set e $$\delta(Y^{j}=y_{k})=\begin{cases}
1 & \text{se } Y^{j}=y_{k} \\
0 & \text{altrimenti}
\end{cases}$$
+ $\sigma^{2}_{ik}$ e' la _varianza di_ $X_{i}$ _per le istanze con etichetta_ $Y=y_{k}$. $$\sigma^{2}_{ik}=\frac{\sum^{}_{j}(X^{j}_{i}-\mu_{ij})^{2}\delta(Y^{j}=y_{k})}{\sum^{}_{j}\delta(Y^{j}=y_{k})}$$
#### Regressione Logistica
L'idea di base e' quella di _cercare di apprendere direttamente_ $P(Y|X)$, senza prima apprendere $P(Y)$ e $P(X|Y)$.

Nel caso di [[Lecture - 15-10-2025-13.39#Na√Øve Bayes|Na√Øve Bayes]], si adotta la seguente formula.
___Supponiamo___:
+ $Y$ variabile aleatoria booleana.
+ $X_{i}$ variabili aleatorie continue.
+ $X_{i}$ sono indipendenti l'una dall'altra data $Y$.
+ $P(X_{i}|Y=k)$ hanno distribuzioni gaussiane $N(\mu_{ik}, \sigma_{i})$.
+ $Y$ ha una distribuzione di Bernoulli $\pi$.
___Allora___: $$P(Y=1|X=\langle x_{1},\dots,x_{n} \rangle)=\frac{1}{1+\exp(w_{0}+\sum^{}_{i}w_{i}x_{i})}$$
___Dimostrazione___:
+ Per ipotesi sappiamo che $$P(X_{i}|Y=k)=N(X_{i},\mu_{ik},\sigma_{i})=\frac{1}{\sigma_{i}\sqrt{ 2\pi }}e^{-\frac{(x-\mu_{ik})^{2}}{2\sigma^{2}_{i}}}$$
+ Quindi: $$\begin{align*}
P(Y=1|X)&= \frac{P(Y=1)\cdot P(X|Y=1)}{P(Y=1)\cdot P(X|Y=1)+P(Y=0)\cdot P(X|Y=0)}\\
&= \frac{1}{1+\frac{P(Y=0)\cdot P(X|Y=0)}{P(Y=1)\cdot P(X|Y=1)}}\\
&= \frac{1}{1+\exp\left(\ln\left(\frac{P(Y=0)\cdot P(X|Y=0)}{P(Y=1)\cdot P(X|Y=1)}\right)\right)}\\
&= \frac{1}{1+\exp\left(\ln\left(\frac{1-\pi}{\pi}\right)+\sum^{}_{i}\ln\left(\frac{P(X_{i}|Y=0)}{P(X_{i}|Y=1)}\right)\right)}\\
&= \frac{1}{1+\exp\left(\ln\left(\frac{1-\pi}{\pi}\right)+\sum^{}_{i}(\frac{\mu_{i_{0}}-\mu_{i_{1}}}{\sigma^2_{i}}X_{i}+\frac{\mu^{2}_{i_{1}}-\mu^{2}_{i_{0}}}{2\sigma^{2}_{i}})\right)}
\end{align*}$$
___Nota Interessante___:
+ Se $$P(Y=1|X=\langle x_{1},\dots,x_{n} \rangle)=\frac{1}{1+\exp\left( w_{0}+\sum^{}_{i}w_{i}x_{i} \right)}$$
+ Allora $$P(Y=0|X=\langle x_{1},\dots,x_{n} \rangle)=\frac{\exp\left( w_{0}+\sum^{}_{i}w_{i}x_{i} \right)}{1+\exp\left( w_{0}+\sum^{}_{i}w_{i}x_{i} \right)}$$
+ Quindi $$\frac{P(Y=0|X=\langle x_{1},\dots,x_{n} \rangle)}{P(Y=1|X=\langle x_{1},\dots,x_{n} \rangle)} = \exp\left( w_{0}+\sum^{}_{i}w_{i}x_{i} \right)$$e in particolare $$\ln\left( \frac{P(Y=0|X=\langle x_{1},\dots,x_{n} \rangle)}{P(Y=1|X=\langle x_{1},\dots,x_{n} \rangle)} \right) = w_{0}+\sum^{}_{i}w_{i}x_{i}$$
+ Si evince che _per classificare_ $X=\langle x_{1},\dots,x_{n} \rangle$ _e' sufficiente vedere se_ $w_{0}+\sum^{}_{i}w_{i}x_{i}>0$.

La regressione logistica assume che $$P(Y=1|X=\langle x_{1},\dots,x_{n} \rangle)=\frac{1}{1+\exp\left( -w_{0}-\sum^{}_{i}w_{i}x_{i} \right)}$$_e cerca di stimare direttamente i parametri_ $w_{i}$.
+ La funzione $$\sigma(x)=\frac{1}{1+e^{-x}}$$e' detta _funzione logistica_.
+ Restituisce un valore nell'intervallo $\{ 0,\dots, 1 \}$ e _trasforma un valore numerico in una probabilita'_.
#### Training Per Regressione Logistica Nel Caso Binario
Sappiamo che $$P(y=1|x,w)=\sigma\left( w_{0}+\sum^{}_{i}w_{i}x_{i} \right)$$
Date _tutte le coppie_, _indipendenti_, di _inputs_ $\langle x^{l},y^{l} \rangle$, la loro _probabilita' e'_ $$\prod^{}_{l}P(y^{l}|x^{l},w)=\prod^{}_{l}P(y^{l}=1|x^{l},w)^{y^{l}}\cdot P(y^{l}=0|x^{l}_{i}w_{i})^{1-y^{l}}$$
Vogliamo trovare i _valori dei parametri_ $w$ _che massimizzano questa probabilita'_ ([[Lecture - 15-10-2025-13.39#Riguardo La Maximum Likelihood|MLE]]).
Possiamo quindi _passare ai logaritmi e massimizzare_: $$\sum^{}_{l}\log(P(y^{l}|x^{l},w)) = \sum^{}_{l}(y^{l}\cdot \log(P(Y=1|x^{l},w))+(1-y^{l})\cdot \log(P(Y=0|x^{l},w)))$$
Per questo problema di ottimizzazione, _non esiste una soluzione analitica_, e quindi _occorre utilizzare dei metodi iterativi di ottimizzazione_, come la _tecnica del gradiente_.

## ‚ùì Domande
---

## üí° Riferimenti
---

## üß© Tasks
---
+ [ ] Review [[Lecture - 16-10-2025-13.09]]

```button 
name ‚úÖ Mark [[Lecture - 16-10-2025-13.09]] As Reviewed 
type command 
action QuickAdd: Mark As Reviewed
```
