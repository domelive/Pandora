---
date: 2025-10-16 13.09
course: 02 Courses/Anno III/Apprendimento Automatico/Lectures
tags: lecture
reviewed: false
last_reviewed:
next_review:
---
## 🧠 Concetti
---
#### Naive Bayes Gaussiano
+ Distribuzione Gaussiana: $p(x|\mu,\sigma)=\frac{1}{\sigma \sqrt{ 2\pi }}e^{-\frac{(x-\mu)^{2}}{2\sigma^2}}$.
+ Valore medio: $E[X] = \mu$.
+ Varianza: $Var[X] = \sigma^2$.
	+ Indica qual'è l'errore medio rispetto alla media.
+ Deviazione standard: $\sigma_{x}=\sigma$.
#### Accuratezza, Precisione E Richiamo
+ _Accuratezza_: quante istanze sono classificate correttamente: $$Accuratezza = \frac{TP+TN}{All}$$
	+ Può essere forviante.
		+ Ad esempio quando abbiamo dei data set sbilanciati.
+ _Precisione_: quanto è precisa la classificazione sui positivi: $$Precisione = \frac{TP}{TP+FP}$$
+ _Richiamo_: che percentuale dei positivi è recuperata: $$Richiamo = \frac{TP}{TP + FN}$$
+ _Media armonica_ tra $Precisione$ e $Richiamo$: $$F_{1} = 2\frac{Precisione \cdot Richiamo}{Precisione + Richiamo}$$
#### Parametri Descrittivi Del Modello
+ Assumiamo che ogni valore della variabile aleatoria abbia una distribuzione gaussiana.
+ _Apprendimento_: stimare i valori dei parametri $\mu_{ik}, \sigma_{ik}$ e $\pi_{k}=P(Y=y_{k})$
+ _Classificazione_ di un nuovo parametro $x^{new} = \langle a_{1},\dots,a_{n} \rangle$: $$\begin{align*}
Y^{new} &= \arg \max_{y_{k}}P(Y=y_{k})\cdot \prod^{}_{i}P(X_{i}=a_{i}|Y=y_{k}) \\ &= \dots\end{align*}$$
#### Regressione Logistica
+ Cerchiamo di apprendere direttamente $P(Y|X)$, senza prima apprendere $P(Y)$ e $P(X|Y)$ con Naive Bayes.
+ Come possiamo esprimere in modo parametrico la probabilità $P(Y|X)$?
	+ Cercare delle distribuzioni matematiche di probabilità che potrebbero riflettere la probabilità che cerchiamo in funzione di $X$.
+ Supponiamo:
	+ $Y$ è una variabile aleatoria booleana.
	+ $X_{i}$ sono variabili aleatorie continue.
	+ $X_{i}$ sono indipendenti l'una dall'altra data $Y$.
	+ Le probabilità $P(X_{i}|Y=k)$ hanno distribuzione gaussiana.
	+ $Y$ ha una distribuzione di Bernoulli ($\pi$).
+ Allora: $$P(Y=1|X=\langle x_{1},\dots,x_{n} \rangle)=\frac{1}{1+\exp\left( w_{0}+\sum^{}_{i}w_{i}x_{i} \right)}$$
	+ Dimostrazione: ...
+ Per classificare $X = \langle x_{1},\dots,x_{n} \rangle$ è sufficiente vedere se $w_{0}+\sum^{}_{i}w_{i}x_{i} > 0$.
	+ Vedere tutti i passaggi...
+ Funzione logistica: compresa tra $1$ e $0$ --> $\sigma(x)=\frac{1}{1+e^{-x}}$.
	+ Trasforma un valore numerico in una probabilità.
+ Training:
	+ Assumendo che i sample siano tutti indipendenti l'uno dall'altro.
	+ La probabilità è il prodotto delle probabilità sui tutti i dati del data set. (?)


## ❓ Domande
---

## 💡 Riferimenti
---

## 🧩 Tasks
---
+ [ ] Riscrivere [[Lecture - 16-10-2025-13.09]]
+ [ ] Review [[Lecture - 16-10-2025-13.09]]

```button 
name ✅ Mark [[Lecture - 16-10-2025-13.09]] As Reviewed 
type command 
action QuickAdd: Mark As Reviewed
```
